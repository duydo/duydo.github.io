<!doctype html><html lang=en dir=auto data-theme=light><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A Production Celery Worker Freeze Caused by epoll | Duy Do</title><meta name=keywords content="celery,celery-worker,rabbitmq,epoll,linux,linux-kernel,production-incident,debugging,distributed-systems,tcp,haproxy,sre,system-architecture,postmortem"><meta name=description content="Back in mid-2019, while I was working on the Data Platform team at Krom, I ran into a problem that lingered long after it was fixed. Our Celery workers would occasionally stop processing tasks without any obvious signal.
Supervisor still showed them as RUNNING, with uptimes measured in days, but zero tasks were processed. Data pipelines quietly stalled, alerts never fired, and the only clue was an occasional Broken pipe buried in the logs."><meta name=author content="Duy Do"><link rel=canonical href=https://duydo.me/a-production-celery-worker-freeze-caused-by-epoll/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://duydo.me/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://duydo.me/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://duydo.me/favicon-32x32.png><link rel=apple-touch-icon href=https://duydo.me/apple-touch-icon.png><link rel=mask-icon href=https://duydo.me/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://duydo.me/a-production-celery-worker-freeze-caused-by-epoll/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"&&(document.querySelector("html").dataset.theme="dark")</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-MWQHCX1NZ9"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-MWQHCX1NZ9")</script><meta property="og:url" content="https://duydo.me/a-production-celery-worker-freeze-caused-by-epoll/"><meta property="og:site_name" content="Duy Do"><meta property="og:title" content="A Production Celery Worker Freeze Caused by epoll"><meta property="og:description" content="Back in mid-2019, while I was working on the Data Platform team at Krom, I ran into a problem that lingered long after it was fixed. Our Celery workers would occasionally stop processing tasks without any obvious signal.
Supervisor still showed them as RUNNING, with uptimes measured in days, but zero tasks were processed. Data pipelines quietly stalled, alerts never fired, and the only clue was an occasional Broken pipe buried in the logs."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-28T10:03:00+07:00"><meta property="article:modified_time" content="2026-01-28T10:03:00+07:00"><meta property="article:tag" content="Celery"><meta property="article:tag" content="Celery-Worker"><meta property="article:tag" content="Rabbitmq"><meta property="article:tag" content="Epoll"><meta property="article:tag" content="Linux"><meta property="article:tag" content="Linux-Kernel"><meta property="og:image" content="https://duydo.me/images/posts/celery-haproxy-architecture.svg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://duydo.me/images/posts/celery-haproxy-architecture.svg"><meta name=twitter:title content="A Production Celery Worker Freeze Caused by epoll"><meta name=twitter:description content="Back in mid-2019, while I was working on the Data Platform team at Krom, I ran into a problem that lingered long after it was fixed. Our Celery workers would occasionally stop processing tasks without any obvious signal.
Supervisor still showed them as RUNNING, with uptimes measured in days, but zero tasks were processed. Data pipelines quietly stalled, alerts never fired, and the only clue was an occasional Broken pipe buried in the logs."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://duydo.me/posts/"},{"@type":"ListItem","position":2,"name":"A Production Celery Worker Freeze Caused by epoll","item":"https://duydo.me/a-production-celery-worker-freeze-caused-by-epoll/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A Production Celery Worker Freeze Caused by epoll","name":"A Production Celery Worker Freeze Caused by epoll","description":"Back in mid-2019, while I was working on the Data Platform team at Krom, I ran into a problem that lingered long after it was fixed. Our Celery workers would occasionally stop processing tasks without any obvious signal.\nSupervisor still showed them as RUNNING, with uptimes measured in days, but zero tasks were processed. Data pipelines quietly stalled, alerts never fired, and the only clue was an occasional Broken pipe buried in the logs.\n","keywords":["celery","celery-worker","rabbitmq","epoll","linux","linux-kernel","production-incident","debugging","distributed-systems","tcp","haproxy","sre","system-architecture","postmortem"],"articleBody":"Back in mid-2019, while I was working on the Data Platform team at Krom, I ran into a problem that lingered long after it was fixed. Our Celery workers would occasionally stop processing tasks without any obvious signal.\nSupervisor still showed them as RUNNING, with uptimes measured in days, but zero tasks were processed. Data pipelines quietly stalled, alerts never fired, and the only clue was an occasional Broken pipe buried in the logs.\nI kept coming back to this incident long after that. The workaround we deployed was effective, but the root cause turned out to sit below the application layer, in the Linux kernel itself.\nI’m writing this not just as a record of what happened, but because the issue hasn’t really gone away. Systems in production that depend on Celery and RabbitMQ still run into the same failure mode. Six years later, Celery Issue #3773 remains open, and the kernel behavior is unchanged.\nThis is the story of how I traced the problem down to epoll using tools like strace, /proc, lsof, and packet captures and why the final fix was architectural, not code-level, with HAProxy playing a central role.\n1. Silent Workers and a Broken Pipe It always started with the same error in the logs, usually when a worker tried to acknowledge a finished task:\n[2019-07-26 03:41:35,738 - celery - CRITICAL] Couldn't ack 6209, reason:error(32, 'Broken pipe') Yet, when I checked Supervisor, the process looked perfectly healthy:\n$ sudo supervisorctl status goku-worker RUNNING pid 26738, uptime 11 days, 2:19:10 The process (PID 26738) was running but functionally frozen, acting as a true “zombie” process.\n2. Inspecting the Process with strace I needed to see what the worker was actually doing at the kernel level. Attaching strace immediately showed the problem area:\n$ sudo strace -p 26738 -c strace: Process 26738 attached % time seconds usecs/call calls errors syscall ------ ----------- ----------- --------- --------- ---------------- 62.16 0.000023 1 25 epoll_wait ... The worker was spending 62% of its time in epoll_wait. It was waiting for an event that would never come.\nRunning strace -f exposed the futile loop:\n$ sudo strace -p 26738 -f ... [pid 26738] recvfrom(5, 0x7fbead4995c4, 7, 0, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable) [pid 26738] clock_gettime(CLOCK_MONOTONIC, {3399616, 441000388}) = 0 [pid 26738] clock_gettime(CLOCK_MONOTONIC, {3399616, 441208787}) = 0 [pid 26738] epoll_wait(15, [], 64, 502) = 0 [pid 26738] clock_gettime(CLOCK_MONOTONIC, {3399616, 944133989}) = 0 [pid 26738] recvfrom(21, 0x7fbead4995c4, 7, 0, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable) [pid 26738] clock_gettime(CLOCK_MONOTONIC, {3399616, 944706657}) = 0 [pid 26738] clock_gettime(CLOCK_MONOTONIC, {3399616, 944870229}) = 0 [pid 26738] epoll_wait(15, [], 64, 999) = 0 [pid 26738] clock_gettime(CLOCK_MONOTONIC, {3399617, 944471272}) = 0 [pid 26738] clock_gettime(CLOCK_MONOTONIC, {3399617, 944612143}) = 0 [pid 26738] epoll_wait(15, [], 64, 1) = 0 [pid 26738] clock_gettime(CLOCK_MONOTONIC, {3399617, 945931383}) = 0 [pid 26738] recvfrom(21, 0x7fbead4995c4, 7, 0, NULL, NULL) = -1 EAGAIN (Resource temporarily unavailable) ... The worker kept trying to recvfrom on FD 5 and FD 21, epoll_wait on FD 15, timing out, and going back to sleep. It was waiting on dead sockets.\n3. Mapping File Descriptors to RabbitMQ To prove these were network connections, I checked the file descriptor symbolic links:\n$ sudo ls -la /proc/26738/fd/5 lrwx------ 1 xxx xxx 64 Aug 2 06:05 /proc/26738/fd/5 -\u003e socket:[99157475] $ sudo ls -la /proc/26738/fd/21 lrwx------ 1 xxx xxx 64 Aug 2 06:05 /proc/26738/fd/21 -\u003e socket:[99144296] $ sudo ls -la /proc/26738/fd/15 lrwx------ 1 xxx xxx 64 Aug 2 06:05 /proc/26738/fd/15 -\u003e anon_inode:[eventpoll] FD 5 and FD 21 were clearly sockets, and FD 15 was the epoll instance managing them.\nUsing lsof confirmed they pointed straight at our RabbitMQ broker:\n$ sudo lsof -p 26738 | grep 99157475 celery 26738 xxx 5u IPv4 99157475 0t0 TCP xxx-1084:50954-\u003erabbit.xxx-1084:amqp (ESTABLISHED) $ sudo lsof -p 26738 | grep 99144296 celery 26738 xxx 21u IPv4 99144296 0t0 TCP xxx-1084:38194-\u003erabbit.xxx-1084:amqp (ESTABLISHED) The kernel insisted the connection was ESTABLISHED. But a final look at the TCP queues told the real story:\n$ sudo head -n1 /proc/26738/net/tcp; grep -a 99157475 /proc/26738/net/tcp sl local_address rem_address st tx_queue rx_queue tr tm-\u003ewhen retrnsmt uid timeout inode 10: 8A01010A:C70A 5E00010A:1628 01 00000000:00000000 02:00000351 00000000 1005 0 99157475 2 0000000000000000 20 4 30 10 -1 $ sudo head -n1 /proc/26738/net/tcp; grep -a 99144296 /proc/26738/net/tcp sl local_address rem_address st tx_queue rx_queue tr tm-\u003ewhen retrnsmt uid timeout inode 27: 8A01010A:9532 5E00010A:1628 01 00000000:00000000 02:00000B01 00000000 1005 0 99144296 2 0000000000000000 20 4 0 10 -1 Zero bytes in tx/rx queues indicated a ghost connection that was alive in name but dead in function.\n4. An epoll Edge Case I realized that no amount of tweaking Celery or Kombu would help because the problem ran deeper than application code.\nThe core insight came from this analysis: epoll is fundamentally broken 1/2. This is a known, long-standing flaw in Linux’s epoll implementation under edge cases. When RabbitMQ crashes or closes a connection uncleanly (e.g., lost FIN packet), the kernel fails to notify epoll_wait. The socket lingers in ESTABLISHED state where it appears alive in /proc but is dead in reality.\nCelery’s event loop, built on Kombu and epoll, was permanently trapped waiting for an event that would never arrive.\nYou can’t patch the Linux kernel in production. You can’t fork Celery. You work around it.\n5. Working Around It with HAProxy My solution was to introduce HAProxy as TCP proxy sitting between the Celery workers and RabbitMQ.\nWhy this worked where code failed: Forced disconnects: HAProxy is better at enforcing TCP health. I set strict timeout client and timeout server values. When RabbitMQ failed, HAProxy detected the failure and actively sent a clean RST packet to the Celery worker. Bypassing the flaw: This clean disconnect forced the Celery worker out of the epoll_wait hang state with an explicit error, allowing its recovery logic to fire immediately and reconnect cleanly. Lessons Learned Don’t trust the process status: A RUNNING process in Supervisor doesn’t mean it’s working. Check the internal metrics or logs. When logs go silent, go lower: strace, lsof, and /proc are your best friends when application logs go silent. Solve at the right layer: Sometimes the fix isn’t in the code you write, but in the infrastructure you deploy. I learned the hard way that not every production issue has a fix in code. Some bugs live below your stack, and the only thing you can do is design around them. Adding HAProxy didn’t solve epoll, but it stopped the workers from getting stuck and in production, that was what mattered.\n","wordCount":"1081","inLanguage":"en","image":"https://duydo.me/images/posts/celery-haproxy-architecture.svg","datePublished":"2026-01-28T10:03:00+07:00","dateModified":"2026-01-28T10:03:00+07:00","author":{"@type":"Person","name":"Duy Do"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://duydo.me/a-production-celery-worker-freeze-caused-by-epoll/"},"publisher":{"@type":"Organization","name":"Duy Do","logo":{"@type":"ImageObject","url":"https://duydo.me/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://duydo.me/ accesskey=h title="Duy Do (Alt + H)">Duy Do</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://duydo.me/archives/ title=Archives><span>Archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">A Production Celery Worker Freeze Caused by epoll</h1><div class=post-meta><span title='2026-01-28 10:03:00 +0700 +07'>Jan 28, 2026</span>&nbsp;·&nbsp;<span>Duy Do</span></div></header><figure class=entry-cover><img loading=eager src=https://duydo.me/images/posts/celery-haproxy-architecture.svg alt></figure><div class=post-content><p>Back in mid-2019, while I was working on the Data Platform team at Krom, I ran into a problem that lingered long after it was fixed. Our Celery workers would occasionally stop processing tasks without any obvious signal.</p><p>Supervisor still showed them as <code>RUNNING</code>, with uptimes measured in days, but zero tasks were processed. Data pipelines quietly stalled, alerts never fired, and the only clue was an occasional <code>Broken pipe</code> buried in the logs.</p><p>I kept coming back to this incident long after that. The workaround we deployed was effective, but the root cause turned out to sit below the application layer, in the Linux kernel itself.</p><p>I’m writing this not just as a record of what happened, but because the issue hasn’t really gone away. Systems in production that depend on Celery and RabbitMQ still run into the same failure mode. Six years later, <a href=https://github.com/celery/celery/issues/3773>Celery Issue #3773</a> remains open, and the kernel behavior is unchanged.</p><p>This is the story of how I traced the problem down to <code>epoll</code> using tools like <code>strace</code>, <code>/proc</code>, <code>lsof</code>, and packet captures and why the final fix was architectural, not code-level, with HAProxy playing a central role.</p><h3 id=1-silent-workers-and-a-broken-pipe>1. Silent Workers and a Broken Pipe<a hidden class=anchor aria-hidden=true href=#1-silent-workers-and-a-broken-pipe>#</a></h3><p>It always started with the same error in the logs, usually when a worker tried to acknowledge a finished task:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=o>[</span>2019-07-26 03:41:35,738 - celery - CRITICAL<span class=o>]</span> Couldn<span class=s1>&#39;t ack 6209, reason:error(32, &#39;</span>Broken pipe<span class=err>&#39;</span><span class=o>)</span>
</span></span></code></pre></div><p>Yet, when I checked <code>Supervisor</code>, the process looked perfectly healthy:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>$ sudo supervisorctl status
</span></span><span class=line><span class=cl>goku-worker                      RUNNING   pid 26738, uptime <span class=m>11</span> days, 2:19:10
</span></span></code></pre></div><p>The process (<code>PID 26738</code>) was running but functionally frozen, acting as a true &ldquo;zombie&rdquo; process.</p><h3 id=2-inspecting-the-process-with-strace>2. Inspecting the Process with <code>strace</code><a hidden class=anchor aria-hidden=true href=#2-inspecting-the-process-with-strace>#</a></h3><p>I needed to see what the worker was actually doing at the kernel level. Attaching <code>strace</code> immediately showed the problem area:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>$ sudo strace -p <span class=m>26738</span> -c
</span></span><span class=line><span class=cl>strace: Process <span class=m>26738</span> attached
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>% <span class=nb>time</span>     seconds  usecs/call     calls    errors syscall
</span></span><span class=line><span class=cl>------ ----------- ----------- --------- --------- ----------------
</span></span><span class=line><span class=cl> 62.16    0.000023           <span class=m>1</span>        <span class=m>25</span>           epoll_wait
</span></span><span class=line><span class=cl>...
</span></span></code></pre></div><p>The worker was spending 62% of its time in <code>epoll_wait</code>. It was waiting for an event that would never come.</p><p>Running <code>strace -f</code> exposed the futile loop:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>$ sudo strace -p <span class=m>26738</span> -f
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl><span class=o>[</span>pid 26738<span class=o>]</span> recvfrom<span class=o>(</span>5, 0x7fbead4995c4, 7, 0, NULL, NULL<span class=o>)</span> <span class=o>=</span> -1 EAGAIN <span class=o>(</span>Resource temporarily unavailable<span class=o>)</span>
</span></span><span class=line><span class=cl><span class=o>[</span>pid 26738<span class=o>]</span> clock_gettime<span class=o>(</span>CLOCK_MONOTONIC, <span class=o>{</span>3399616, 441000388<span class=o>})</span> <span class=o>=</span> <span class=m>0</span>
</span></span><span class=line><span class=cl><span class=o>[</span>pid 26738<span class=o>]</span> clock_gettime<span class=o>(</span>CLOCK_MONOTONIC, <span class=o>{</span>3399616, 441208787<span class=o>})</span> <span class=o>=</span> <span class=m>0</span>
</span></span><span class=line><span class=cl><span class=o>[</span>pid 26738<span class=o>]</span> epoll_wait<span class=o>(</span>15, <span class=o>[]</span>, 64, 502<span class=o>)</span> <span class=o>=</span> <span class=m>0</span>
</span></span><span class=line><span class=cl><span class=o>[</span>pid 26738<span class=o>]</span> clock_gettime<span class=o>(</span>CLOCK_MONOTONIC, <span class=o>{</span>3399616, 944133989<span class=o>})</span> <span class=o>=</span> <span class=m>0</span>
</span></span><span class=line><span class=cl><span class=o>[</span>pid 26738<span class=o>]</span> recvfrom<span class=o>(</span>21, 0x7fbead4995c4, 7, 0, NULL, NULL<span class=o>)</span> <span class=o>=</span> -1 EAGAIN <span class=o>(</span>Resource temporarily unavailable<span class=o>)</span>
</span></span><span class=line><span class=cl><span class=o>[</span>pid 26738<span class=o>]</span> clock_gettime<span class=o>(</span>CLOCK_MONOTONIC, <span class=o>{</span>3399616, 944706657<span class=o>})</span> <span class=o>=</span> <span class=m>0</span>
</span></span><span class=line><span class=cl><span class=o>[</span>pid 26738<span class=o>]</span> clock_gettime<span class=o>(</span>CLOCK_MONOTONIC, <span class=o>{</span>3399616, 944870229<span class=o>})</span> <span class=o>=</span> <span class=m>0</span>
</span></span><span class=line><span class=cl><span class=o>[</span>pid 26738<span class=o>]</span> epoll_wait<span class=o>(</span>15, <span class=o>[]</span>, 64, 999<span class=o>)</span> <span class=o>=</span> <span class=m>0</span>
</span></span><span class=line><span class=cl><span class=o>[</span>pid 26738<span class=o>]</span> clock_gettime<span class=o>(</span>CLOCK_MONOTONIC, <span class=o>{</span>3399617, 944471272<span class=o>})</span> <span class=o>=</span> <span class=m>0</span>
</span></span><span class=line><span class=cl><span class=o>[</span>pid 26738<span class=o>]</span> clock_gettime<span class=o>(</span>CLOCK_MONOTONIC, <span class=o>{</span>3399617, 944612143<span class=o>})</span> <span class=o>=</span> <span class=m>0</span>
</span></span><span class=line><span class=cl><span class=o>[</span>pid 26738<span class=o>]</span> epoll_wait<span class=o>(</span>15, <span class=o>[]</span>, 64, 1<span class=o>)</span>   <span class=o>=</span> <span class=m>0</span>
</span></span><span class=line><span class=cl><span class=o>[</span>pid 26738<span class=o>]</span> clock_gettime<span class=o>(</span>CLOCK_MONOTONIC, <span class=o>{</span>3399617, 945931383<span class=o>})</span> <span class=o>=</span> <span class=m>0</span>
</span></span><span class=line><span class=cl><span class=o>[</span>pid 26738<span class=o>]</span> recvfrom<span class=o>(</span>21, 0x7fbead4995c4, 7, 0, NULL, NULL<span class=o>)</span> <span class=o>=</span> -1 EAGAIN <span class=o>(</span>Resource temporarily unavailable<span class=o>)</span>
</span></span><span class=line><span class=cl>...
</span></span></code></pre></div><p>The worker kept trying to <code>recvfrom</code> on <code>FD 5</code> and <code>FD 21</code>, <code>epoll_wait</code> on <code>FD 15</code>, timing out, and going back to sleep. It was waiting on dead sockets.</p><h3 id=3-mapping-file-descriptors-to-rabbitmq>3. Mapping File Descriptors to RabbitMQ<a hidden class=anchor aria-hidden=true href=#3-mapping-file-descriptors-to-rabbitmq>#</a></h3><p>To prove these were network connections, I checked the file descriptor symbolic links:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>$ sudo ls -la /proc/26738/fd/5
</span></span><span class=line><span class=cl>lrwx------ <span class=m>1</span> xxx xxx <span class=m>64</span> Aug  <span class=m>2</span> 06:05 /proc/26738/fd/5 -&gt; socket:<span class=o>[</span>99157475<span class=o>]</span>
</span></span><span class=line><span class=cl>$ sudo ls -la /proc/26738/fd/21
</span></span><span class=line><span class=cl>lrwx------ <span class=m>1</span> xxx xxx <span class=m>64</span> Aug  <span class=m>2</span> 06:05 /proc/26738/fd/21 -&gt; socket:<span class=o>[</span>99144296<span class=o>]</span>
</span></span><span class=line><span class=cl>$ sudo ls -la /proc/26738/fd/15
</span></span><span class=line><span class=cl>lrwx------ <span class=m>1</span> xxx xxx <span class=m>64</span> Aug  <span class=m>2</span> 06:05 /proc/26738/fd/15 -&gt; anon_inode:<span class=o>[</span>eventpoll<span class=o>]</span>
</span></span></code></pre></div><p><code>FD 5</code> and <code>FD 21</code> were clearly sockets, and <code>FD 15</code> was the epoll instance managing them.</p><p>Using <code>lsof</code> confirmed they pointed straight at our RabbitMQ broker:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>$ sudo lsof -p <span class=m>26738</span> <span class=p>|</span> grep <span class=m>99157475</span>
</span></span><span class=line><span class=cl>celery  <span class=m>26738</span> xxx    5u     IPv4 <span class=m>99157475</span>      0t0      TCP xxx-1084:50954-&gt;rabbit.xxx-1084:amqp <span class=o>(</span>ESTABLISHED<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>$ sudo lsof -p <span class=m>26738</span> <span class=p>|</span> grep <span class=m>99144296</span>
</span></span><span class=line><span class=cl>celery  <span class=m>26738</span> xxx   21u     IPv4 <span class=m>99144296</span>      0t0      TCP xxx-1084:38194-&gt;rabbit.xxx-1084:amqp <span class=o>(</span>ESTABLISHED<span class=o>)</span>
</span></span></code></pre></div><p>The kernel insisted the connection was <code>ESTABLISHED</code>. But a final look at the TCP queues told the real story:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>$ sudo head -n1 /proc/26738/net/tcp<span class=p>;</span> grep -a <span class=m>99157475</span> /proc/26738/net/tcp
</span></span><span class=line><span class=cl>  sl  local_address rem_address   st tx_queue rx_queue tr tm-&gt;when retrnsmt   uid  timeout inode
</span></span><span class=line><span class=cl>  10: 8A01010A:C70A 5E00010A:1628 <span class=m>01</span> 00000000:00000000 02:00000351 <span class=m>00000000</span>  <span class=m>1005</span>        <span class=m>0</span> <span class=m>99157475</span> <span class=m>2</span> <span class=m>0000000000000000</span> <span class=m>20</span> <span class=m>4</span> <span class=m>30</span> <span class=m>10</span> -1
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>$ sudo head -n1 /proc/26738/net/tcp<span class=p>;</span> grep -a <span class=m>99144296</span> /proc/26738/net/tcp
</span></span><span class=line><span class=cl>  sl  local_address rem_address   st tx_queue rx_queue tr tm-&gt;when retrnsmt   uid  timeout inode
</span></span><span class=line><span class=cl>  27: 8A01010A:9532 5E00010A:1628 <span class=m>01</span> 00000000:00000000 02:00000B01 <span class=m>00000000</span>  <span class=m>1005</span>        <span class=m>0</span> <span class=m>99144296</span> <span class=m>2</span> <span class=m>0000000000000000</span> <span class=m>20</span> <span class=m>4</span> <span class=m>0</span> <span class=m>10</span> -1
</span></span></code></pre></div><p>Zero bytes in tx/rx queues indicated a ghost connection that was alive in name but dead in function.</p><h3 id=4-an-epoll-edge-case>4. An epoll Edge Case<a hidden class=anchor aria-hidden=true href=#4-an-epoll-edge-case>#</a></h3><p>I realized that no amount of tweaking Celery or Kombu would help because the problem ran deeper than application code.</p><p>The core insight came from this analysis: <a href=https://idea.popcount.org/2017-02-20-epoll-is-fundamentally-broken-12/>epoll is fundamentally broken 1/2</a>. This is a known, long-standing flaw in Linux’s epoll implementation under edge cases. When RabbitMQ crashes or closes a connection uncleanly (e.g., lost FIN packet), the kernel fails to notify <code>epoll_wait</code>. The socket lingers in <code>ESTABLISHED</code> state where it appears alive in <code>/proc</code> but is dead in reality.</p><p>Celery’s event loop, built on Kombu and <code>epoll</code>, was permanently trapped waiting for an event that would never arrive.</p><blockquote><p>You can’t patch the Linux kernel in production.
You can’t fork Celery.
You work around it.</p></blockquote><h3 id=5-working-around-it-with-haproxy>5. Working Around It with HAProxy<a hidden class=anchor aria-hidden=true href=#5-working-around-it-with-haproxy>#</a></h3><p>My solution was to introduce HAProxy as TCP proxy sitting between the Celery workers and RabbitMQ.</p><h4 id=why-this-worked-where-code-failed>Why this worked where code failed:<a hidden class=anchor aria-hidden=true href=#why-this-worked-where-code-failed>#</a></h4><ol><li><strong>Forced disconnects:</strong> HAProxy is better at enforcing <code>TCP</code> health. I set strict <code>timeout client</code> and <code>timeout server</code> values. When RabbitMQ failed, HAProxy detected the failure and actively sent a clean <code>RST</code> packet to the <code>Celery</code> worker.</li><li><strong>Bypassing the flaw:</strong> This clean disconnect forced the Celery worker out of the <code>epoll_wait</code> hang state with an explicit error, allowing its recovery logic to fire immediately and reconnect cleanly.</li></ol><h3 id=lessons-learned>Lessons Learned<a hidden class=anchor aria-hidden=true href=#lessons-learned>#</a></h3><ul><li><strong>Don&rsquo;t trust the process status:</strong> A <code>RUNNING</code> process in Supervisor doesn&rsquo;t mean it&rsquo;s working. Check the internal metrics or logs.</li><li><strong>When logs go silent, go lower:</strong> <code>strace</code>, <code>lsof</code>, and <code>/proc</code> are your best friends when application logs go silent.</li><li><strong>Solve at the right layer:</strong> Sometimes the fix isn&rsquo;t in the code you write, but in the infrastructure you deploy.</li></ul><p>I learned the hard way that not every production issue has a fix in code. Some bugs live below your stack, and the only thing you can do is design around them. Adding HAProxy didn’t solve epoll, but it stopped the workers from getting stuck and in production, that was what mattered.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://duydo.me/tags/celery/>Celery</a></li><li><a href=https://duydo.me/tags/celery-worker/>Celery-Worker</a></li><li><a href=https://duydo.me/tags/rabbitmq/>Rabbitmq</a></li><li><a href=https://duydo.me/tags/epoll/>Epoll</a></li><li><a href=https://duydo.me/tags/linux/>Linux</a></li><li><a href=https://duydo.me/tags/linux-kernel/>Linux-Kernel</a></li><li><a href=https://duydo.me/tags/production-incident/>Production-Incident</a></li><li><a href=https://duydo.me/tags/debugging/>Debugging</a></li><li><a href=https://duydo.me/tags/distributed-systems/>Distributed-Systems</a></li><li><a href=https://duydo.me/tags/tcp/>Tcp</a></li><li><a href=https://duydo.me/tags/haproxy/>Haproxy</a></li><li><a href=https://duydo.me/tags/sre/>Sre</a></li><li><a href=https://duydo.me/tags/system-architecture/>System-Architecture</a></li><li><a href=https://duydo.me/tags/postmortem/>Postmortem</a></li></ul><nav class=paginav><a class=next href=https://duydo.me/honored-to-be-an-elastic-contributor/><span class=title>Next »</span><br><span>Honored to Be an Elastic Contributor</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://duydo.me/>Duy Do</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>